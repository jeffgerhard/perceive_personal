{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Introduction\n",
    "\n",
    "The PERCEIVE project seeks to proactively identify upcoming cybersecurity threats through textual similarity. Social network analysis, however, can be used to partition textual content, and hence serve as social-oriented word selection criteria, for defining corpus' documents. \n",
    "\n",
    "The subject of this notebook, the [Full Disclosure (FD) mailing list](http://seclists.org/fulldisclosure/) is a \"public, vendor-neutral forum for detailed discussion of vulnerabilities and exploitation techniques, as well as tools, papers, news, and events of interest to the community.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "Although cybersecurity email mailing lists provide a rich source to identify emerging concepts, it contains a large amount of content that is irrelevant to the project's purpose, including but not limited to:\n",
    "\n",
    " * conference invitations[[1](http://seclists.org/fulldisclosure/2017/Feb/6)]\n",
    " * vendor announcements[[2](http://seclists.org/fulldisclosure/2016/Dec/48)]\n",
    " * extensive conversations on security topics[[3](http://seclists.org/fulldisclosure/2004/Jul/1026)]\n",
    " * a significant amount of trolling[[4](http://seclists.org/fulldisclosure/2008/Apr/96)] \n",
    " * and nonsense[[5](http://seclists.org/fulldisclosure/2009/Jul/289)] [[6](http://seclists.org/fulldisclosure/2004/Jul/796)].\n",
    "\n",
    "As some of the irrelevant content can be strictly tied to it's producer, i.e., the **authors** of the **e-mail replies**, Social network analysis provides an interesting opportunity for the isolation of relevant discussion topics in order to **filter** non-related vulnerability content. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Definition\n",
    "\n",
    "Earlier in this project, the FD email lists were developed into networks of edges and nodes, divided by year. The original csv files are available [online](https://mega.nz/#F!CUEByR5I!GY56GzTpYz68IlTqj4aQNQ!fR8jFLxL). The networks were imported into [Gephi](https://gephi.org/) to create a series of [visualization graphics](\n",
    "https://mega.nz/#F!btdgFBID!hktkVrhZB5etOBBVLrgTrA).\n",
    "\n",
    "In the FD graphs, nodes represent either documents (i.e., emails) or authors. Edges are directed, representing authorship and replies; edge weight indicates an increasing number of replies. In the Gephi graphics, blue nodes represent authors and red nodes represent documents. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Disclosure Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Considered Tools\n",
    "\n",
    "### Gephi \n",
    "\n",
    "While Gephi has provided useful for data exploration, some difficulties arised. In particular, many of the core functions[[7](https://github.com/jaroslav-kuchar/Social-Network-Analysis/issues/2)] and plugins[[8](https://github.com/gephi/gephi/issues/1481)] once used for social network analysis are not compatible with current (0.9) software versions. Gephi specifications note that the 0.9.0 version (December 2015) \"Removed [the] Clustering module\"[[9](https://github.com/gephi/gephi/releases/tag/v0.9.0)] that these plugins relied upon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python-igraph \n",
    "\n",
    "[Python-igraph](http://igraph.org/python/) (igraph hereafter) allows a user to import graphs in a variety of file formats, several of which match Gephi's export options. Brief tests demonstrated that the [GraphML](http://graphml.graphdrawing.org/) format was most successful for a workflow of transferring networks from Gephi to igraph. (Attempt to import [GML](https://en.wikipedia.org/wiki/Graph_Modelling_Language) and [Pajek](http://mrvar.fdv.uni-lj.si/pajek/) caused one instance of Python to hang; further investigation would be required to identify a cause.) The following examples use [GraphML files](https://mega.nz/#F!Dpdh2TjD!4Rd462mFXbdFn5Scs1WwUA) exported from Gephi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IGRAPH D-W- 9793 6463 -- 2008 Full Disclosure network\n",
      "+ attr: name (g), Color (v), Label1 (v), Label2 (v), b (v), g (v), id (v), label (v), nodeType (v), r (v), size (v), x (v), y (v), Edge Label (e), id (e), weight (e)\n"
     ]
    }
   ],
   "source": [
    "# GraphML files may be imported into igraph with a single command (`Read_GraphML`) and then analyzed:\n",
    "from igraph import Graph, summary\n",
    "# import the GraphML files previously exported from Gephi\n",
    "ml = Graph.Read_GraphML('data/2008.graphml')\n",
    "\n",
    "ml['name'] = '2008 Full Disclosure network'\n",
    "# summary reports the edges, nodes, and overall attributes in use. \n",
    "summary(ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The summary command providing the output seen above is explained in the igraph documentation[[10](http://igraph.org/python/doc/igraph.summary'.GraphSummary-class.html)]. In this example, the four-character code \"D-W-\" indicates that the graph is directed and weighted. The graph has 9793 vertices (nodes) and 6463 edges. \n",
    "\n",
    "The attributes given in the summary (\"name\", \"Color\", etc.) are those for the graph (g), vertices (v), or edges (e). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID list of first two vertices in network:\n",
      "['andur matrix <andurmatrix () gmail com>', 'Adam Muntner <adam.muntner () quietmove com>']\n"
     ]
    }
   ],
   "source": [
    "# show the id attribute of the first 2 vertices\n",
    "print('ID list of first two vertices in network:')\n",
    "print(ml.vs[0:2]['id'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nodes can either be **authors** as shown above, or **e-mail threads** (in which case their identifier is the year, month and relative id for the given month)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IDs of vertices with highest degree: ['security () mandriva com']\n"
     ]
    }
   ],
   "source": [
    "# identify vertices with the highest degree and betweenness\n",
    "# (This sample is borrowed directly from the tutorial!)\n",
    "print('\\nIDs of vertices with highest degree:', ml.vs.select(_degree = ml.maxdegree())['id'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often common that higher degree nodes (those who post several e-mail replies in e-mail threads) are advisories, and do not really communicate. We can clearly see that the author above, by his e-mail, is clearly a research group in the mailing list, and those are more related to advisories per our manual observations. \n",
    "\n",
    "##### Cleaning up the graph\n",
    "\n",
    "It's worth noting that the attributes of \"r\", \"g\", and \"b\" were accidentally introduced in Gephi (as colors) and it may be helpful to delete them. We don't need the size or position data either (\"x\", \"y\", \"size\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total vertices: 9793 including  2438 authors\n",
      "\n",
      "Filtering out authors with neighbors mostly of in-degree 1...\n",
      "total vertices: 9421 including  2066 authors\n",
      "\n",
      "Filtering out documents or authors with degree 0...\n",
      "total vertices: 5889 including  1474 authors\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# delete unneeded attributes(color, size, position)\n",
    "\n",
    "for _ in ['r', 'g', 'b', 'x', 'y', 'size']:\n",
    "    del ml.vs[_]\n",
    "    \n",
    "# there is also a gephi color-import plugin that expects a value of \"color\" or \"colour\" in the attributes.\n",
    "# let's rename the \"Color\" attribute to \"color\"\n",
    "\n",
    "ml.vs['color'] = ml.vs['Color']\n",
    "del ml.vs['Color']\n",
    "\n",
    "# FILTERING THE NODES\n",
    "\n",
    "\n",
    "# Q: how many nodes are there anyway?\n",
    "print('total vertices:', len(ml.vs), 'including ', len(ml.vs(nodeType_eq='author')), 'authors')\n",
    "print()\n",
    "# now let's start filtering...\n",
    "# first remove authors with neighbors mostly of in-degree 1\n",
    "print('Filtering out authors with neighbors mostly of in-degree 1...')\n",
    "for node in ml.vs:\n",
    "    if node['nodeType'] == 'author':\n",
    "        indegrees = [ml.degree(_, mode='in') for _ in ml.neighbors(node)]\n",
    "        if indegrees:  # a few authors have 0 neighbors... this is something to investigate later\n",
    "            if sum(indegrees) / max(len(indegrees), 1) < 1.2: #  there are examples where most of the indegrees are 1 but\n",
    "                                                     #  we have a random response, so let's use the mean\n",
    "                ml.delete_vertices(node) #  deleting vertices also deletes all of its edges\n",
    "\n",
    "\n",
    "print('total vertices:', len(ml.vs), 'including ', len(ml.vs(nodeType_eq='author')), 'authors')\n",
    "print()\n",
    "# we still have too many communities, and a lot of isolated documents from \n",
    "# deleting the authors. let's remove all nodes with degree 0\n",
    "print('Filtering out documents or authors with degree 0...')\n",
    "for node in ml.vs:\n",
    "    if ml.degree(node) == 0: \n",
    "        ml.delete_vertices(node)\n",
    "\n",
    "print('total vertices:', len(ml.vs), 'including ', len(ml.vs(nodeType_eq='author')), 'authors')\n",
    "print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Full Disclosure Visualization over Years \n",
    "\n",
    "The GraphML files from Gephi include the ids as _labels_, and so they are included by default when igraph creates visualizations. These may be eliminated by deleting the attribute.\n",
    "\n",
    "Igraph handles visualization through _layouts_. Layouts are separate objects from the graphs themselves; multiple layouts can be created per graph.\n",
    "\n",
    "Visual graphs are created via `plot` commands, however it demands too much time to render, and visualization is deferred to gephi and them edited on an image editor and assembled as a gif of 5s delay for easier visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import the GraphML files previously exported from Gephi\n",
    "# delete label attributes to avoid visual clutter. In this example, the 'label'\n",
    "# attribute is a duplicate of the 'ID' attribute so there is no need to save the label\n",
    "#del ml.vs['label']\n",
    "\n",
    "#layout = ml.layout_graphopt() # create a Fruchterman Reingold layout for the graph\n",
    "\n",
    "#png = plot(ml, 'output/plot.png', layout=layout, bbox=(1000,1000), margin=100,\n",
    "#           edge_curved=True)  # the bbox keyword argument defines the dimensions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The resulting graphic provides a visual overview of the FD 2016 network:\n",
    "\n",
    "![An igraph-generated FD graph for 2016](https://github.com/sailuh/perceive/raw/master/Websites/Project/fulldisclosure_nooverlap_curved.gif)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the network structure varies considerably across the years. This provides an opportunity to partition a network at a given year into several clusters and investigate if the visualized structure has any association to the textual content of a given **e-mail thread** discussion.\n",
    "\n",
    "If such association exists, then we can leverage the **social network structure** in order to simplify the identification of emerging concepts through text alone. For instance, identifying a group of individuals have preference to certain subjects, or are spammers may become a trivial pre-processing stage before textual content is analyzed for emerging concepts.\n",
    "\n",
    "We begin by considering 2 methods to more precisely define how to partition a network at any given year: \n",
    "\n",
    " * Community Detection \n",
    " * Betweenness Centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Community Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the community measures do not support directed graphs, as seen in the warning message below, but the direction can be removed to compare the results of igraph's community funtions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\igraph\\__init__.py:1072: RuntimeWarning: This method was developed for undirected graphs at src\\community.c:1565\n",
      "  membership, _, q = GraphBase.community_leading_eigenvector(self, clusters, **kwds)\n"
     ]
    }
   ],
   "source": [
    "com = ml.community_leading_eigenvector(weights='weight', clusters=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustering attempt, leading eigenvector:\n",
      "Clustering with 5889 elements and 2152 clusters\n"
     ]
    }
   ],
   "source": [
    "ml.to_undirected(combine_edges='max') # eliminate the directionality\n",
    "\n",
    "com = ml.community_leading_eigenvector(weights='weight', clusters=3)\n",
    "print('clustering attempt, leading eigenvector:')\n",
    "summary(com)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more detailed look at the eigenvector clusters follows in two parts. First, a listing of the overall cluster contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2152 clusters.\n",
      "maximum size: 3150\n",
      "minimum size: 1\n",
      "\n",
      "Summary of first 20 clusters:\n",
      "[0] has size of 3150. Vertices: [0, 1, 2, 3, 4] (and 3145 more)\n",
      "[1] has size of 256. Vertices: [34, 767, 1491, 1495, 1583] (and 251 more)\n",
      "[2] has size of 6. Vertices: [35, 1492, 1533, 2170, 2361] (and 1 more)\n",
      "[3] has size of 42. Vertices: [36, 2140, 2234, 2276, 4258] (and 37 more)\n",
      "[4] has size of 7. Vertices: [42, 1409, 4361, 4392, 5049] (and 2 more)\n",
      "[5] has size of 1. Vertices: [44] \n",
      "[6] has size of 1. Vertices: [72] \n",
      "[7] has size of 1. Vertices: [74] \n",
      "[8] has size of 7. Vertices: [75, 1585, 2564, 2679, 3032] (and 2 more)\n",
      "[9] has size of 3. Vertices: [76, 78, 1537] \n",
      "[10] has size of 1. Vertices: [93] \n",
      "[11] has size of 2. Vertices: [98, 1567] \n",
      "[12] has size of 1. Vertices: [118] \n",
      "[13] has size of 1. Vertices: [119] \n",
      "[14] has size of 3. Vertices: [122, 1183, 1600] \n",
      "[15] has size of 4. Vertices: [128, 247, 1618, 1795] \n",
      "[16] has size of 5. Vertices: [133, 997, 2449, 3839, 3840] \n",
      "[17] has size of 78. Vertices: [139, 231, 1296, 1627, 1628] (and 73 more)\n",
      "[18] has size of 18. Vertices: [153, 1650, 1657, 1659, 1693] (and 13 more)\n"
     ]
    }
   ],
   "source": [
    "print(len(com), 'clusters.')\n",
    "print('maximum size:', len(max(com, key=len)))\n",
    "print('minimum size:', len(min(com, key=len)))\n",
    "\n",
    "print('\\nSummary of first 20 clusters:')\n",
    "for i in range(19):\n",
    "    etc = ''\n",
    "    if len(com[i]) > 5:\n",
    "        etc += '(and ' + str(len(com[i]) - 5) + ' more)'\n",
    "    print('[{}] has size of {}. Vertices: {} {}'.format(i, len(com[i]), com[i][0:5], etc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, a report of the first 10 communities' node types and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMMUNITY 0   ( total size: 3150 )\n",
      "\t author :\t Adam Muntner <adam.muntner () quietmove com>\n",
      "\t author :\t Denzity <denzity () gmail com>\n",
      "\t author :\t Marcin Wielgoszewski <marcinw86 () gmail com>\n",
      "\t author :\t Geo. <geoincidents () nls net>\n",
      "\t author :\t coderman <coderman () gmail com>\n",
      "\t author :\t reepex <reepex () gmail com>\n",
      "\t author :\t Javor Ninov <drfrancky () securax org>\n",
      "\t author :\t SilentRunner <silentrunner () hushmail com>\n",
      "\t author :\t php0t <php0t () zorro hu>\n",
      "\t author :\t Simon Smith <simon () snosoft com>\n",
      "\t... TRUNCATED LISTING\n",
      "\n",
      "COMMUNITY 1   ( total size: 256 )\n",
      "\t author :\t security () mandriva com\n",
      "\t author :\t Jeroen Massar <jeroen () unfix org>\n",
      "\t document :\t [ MDKSA-2007:001 ] - Update libmodplug packages\tfix buffer overflow vulnerabilities\n",
      "\t document :\t [ MDKSA-2007:002 ] - Updated kernel packages fix\tmultiple vulnerabilities and bugs\n",
      "\t document :\t [ MDKSA-2007:003 ] - Updated avahi packages fix\tDoS vulnerability\n",
      "\t document :\t [ MDKSA-2007:004 ] - Updated geoip packages fix\tgeoipupdate vulnerability\n",
      "\t document :\t [ MDKSA-2007-005 ] - Updated xorg-x11/XFree86\tpackages fix integer overflow vulnerabilities\n",
      "\t document :\t [ MDKSA-2007:006 ] - Updated OpenOffice.org\tpackages fix WMF vulnerability\n",
      "\t document :\t [ MDKSA-2007:007 ] - Updated nvidia driver\tpackages fix vulnerability\n",
      "\t document :\t [ MDKSA-2007:008 ] - Updated kerberos packages\tfix vulnerability\n",
      "\t... TRUNCATED LISTING\n",
      "\n",
      "COMMUNITY 2   ( total size: 6 )\n",
      "\t author :\t Vic Vandal <vvandal () well com>\n",
      "\t document :\t CarolinaCon 2007 - Call for Speakers/Papers\n",
      "\t document :\t Re: Call For Participants For A Research Study Of\tHacker Culture\n",
      "\t document :\t CarolinaCon 2007 Announcement/Press Release\n",
      "\t document :\t CarolinaCon presentation drafts\n",
      "\t document :\t CarolinaCon 2008 - Call For Papers/Speakers\n",
      "\n",
      "COMMUNITY 3   ( total size: 42 )\n",
      "\t author :\t Luigi Auriemma <aluigi () autistici org>\n",
      "\t document :\t Players disconnection in Simbin racing games\n",
      "\t document :\t Limited format string in Netrek 2.12.0\n",
      "\t document :\t Buffer-overflow in Conquest client 8.2a (svn 691)\n",
      "\t document :\t Crash in Zoidcom 0.6.7\n",
      "\t document :\t Multiple vulnerabilities in Babo Violent 2 2.08.00\n",
      "\t document :\t Multiple vulnerabilities in Live for Speed 0.5X10\n",
      "\t document :\t Unexploitable buffer-overflow in the logging function of the Unreal engine\n",
      "\t document :\t Multiple vulnerabilities in rFactor 1.250\n",
      "\t document :\t Multiple vulnerabilities in Toribash 2.71\n",
      "\t... TRUNCATED LISTING\n",
      "\n",
      "COMMUNITY 4   ( total size: 7 )\n",
      "\t author :\t Asterisk Security Team <security () asterisk org>\n",
      "\t author :\t Asterisk Security Team <security () digium com>\n",
      "\t document :\t AST-2007-020: Resource Exhaustion Vulnerability in Asterisk SIP channel driver\n",
      "\t document :\t AST-2007-021: Crash from invalid/corrupted MIME bodies when using voicemail with IMAP storage\n",
      "\t document :\t AST-2007-023 - SQL Injection Vulnerabilty in\tcdr_addon_mysql\n",
      "\t document :\t AST-2007-025 - SQL Injection issue in\tres_config_pgsql\n",
      "\t document :\t AST-2007-026 - SQL Injection issue in cdr_pgsql\n",
      "\n",
      "COMMUNITY 5   ( total size: 1 )\n",
      "\t author :\t rich cannings <rcannings () gmail com>\n",
      "\n",
      "COMMUNITY 6   ( total size: 1 )\n",
      "\t author :\t b9u4ea <b9u4ea () gmail com>\n",
      "\n",
      "COMMUNITY 7   ( total size: 1 )\n",
      "\t author :\t Reed Arvin <reedarvin () gmail com>\n",
      "\n",
      "COMMUNITY 8   ( total size: 7 )\n",
      "\t author :\t VMware Security team <security () vmware com>\n",
      "\t document :\t VMware ESX server security updates\n",
      "\t document :\t VMSA-2007-0002 VMware ESX security updates\n",
      "\t document :\t VMSA-2007-0003 VMware ESX 3.0.1 and 3.0.0 server\tsecurity updates\n",
      "\t document :\t VMSA-2007-0004 Multiple Denial-of-Service issues\tfixed\n",
      "\t document :\t VMSA-2007-0004.1 Updated: Multiple Denial-of-Service issues fixed and directory traversal vulnerability\n",
      "\t document :\t VMSA-2007-0006 Critical security updates for all supported versions of VMware ESX Server, VMware Server, VMware Workstation, VMware ACE, and VMware Player\n",
      "\n",
      "COMMUNITY 9   ( total size: 3 )\n",
      "\t author :\t Noe Espinoza M. <nespinoza () grupowissen com>\n",
      "\t author :\t Darren Bounds <dbounds () gmail com>\n",
      "\t document :\t Re: Universal PDF XSS After Party(posible\tsolution)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print('COMMUNITY', i, '  ( total size:', len(com[i]), ')')\n",
    "    for _ in com[i][0:10]:\n",
    "        print('\\t', ml.vs[_]['nodeType'], ':\\t', ml.vs[_]['label'])\n",
    "    if len(com[i]) > 10:\n",
    "        print('\\t... TRUNCATED LISTING')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IGRAPH U-W- 3150 4523 -- Subgraph for cluster 0 (blob)\n",
      "+ attr: name (g), Label1 (v), Label2 (v), color (v), id (v), label (v), nodeType (v), Edge Label (e), id (e), weight (e)\n"
     ]
    }
   ],
   "source": [
    "# There are too many clusters here; manual inspection suggests that we are mostly interested in the large cluster.\n",
    "# This corresponds with the giant blob seen in the Gephi visualizations.\n",
    "\n",
    "# Let's move from working with the entire graph to a subgraph including only this community\n",
    "\n",
    "# TO DO: MAKE SURE WE ARE DEALING WITH THE LARGEST CLUSTER; HERE I KNOW IT IS CLUSTER ZERO\n",
    "\n",
    "\n",
    "blob = ml.induced_subgraph(com[0])\n",
    "blob['name'] = 'Subgraph for cluster 0 (blob)'\n",
    "summary(blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustering attempt, leading eigenvector:\n",
      "Clustering with 3150 elements and 6 clusters\n"
     ]
    }
   ],
   "source": [
    "# Now working with the blob, we can re-attempt clustering and bc!\n",
    "\n",
    "com = blob.community_leading_eigenvector(weights='weight', clusters=8)\n",
    "print('clustering attempt, leading eigenvector:')\n",
    "summary(com)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Betweenness Centrality\n",
    "\n",
    "Igraph betweenness measures, like the clustering algorithms, require undirected graphs. Betweenness is returned as a simple list of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bc = blob.betweenness()\n",
    "bc.sort(reverse=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 0 : 100%\n",
      "Node 1 : 48%\n",
      "Node 2 : 39%\n",
      "Node 3 : 37%\n",
      "Node 4 : 36%\n",
      "Node 5 : 35%\n",
      "Node 6 : 35%\n",
      "Node 7 : 35%\n",
      "Node 8 : 34%\n",
      "Node 9 : 33%\n",
      "Node 10 : 32%\n",
      "Node 11 : 31%\n",
      "Node 12 : 19%\n",
      "Node 13 : 17%\n",
      "Node 14 : 17%\n",
      "Node 15 : 16%\n",
      "Node 16 : 15%\n"
     ]
    }
   ],
   "source": [
    "max_bc = max(bc)\n",
    "bc_normalized = [x / max_bc for x in bc]\n",
    "\n",
    "for idx,val in enumerate(bc_normalized[0:17]):\n",
    "    print('Node',idx,':',\"{:.0%}\".format(bc_normalized[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the betweeness centrality quickly drops past the 16th node, which may indicate nodes that often discuss different subjects for contributing e-mail discussions for different \"community of threads\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting and utilizing analyzed graph data\n",
    "\n",
    "Igraph allows users to export graph files for use in other software such as Gephi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add betweenness centrality, pagerank, and clustering info to original graph\n",
    "bc = blob.betweenness()\n",
    "for idx, node in enumerate(blob.vs):\n",
    "    blob.vs[idx]['blob_betweenness'] = bc[idx]\n",
    "    \n",
    "pr = blob.pagerank(directed=False)\n",
    "for idx, node in enumerate(blob.vs):\n",
    "    blob.vs[idx]['blob_pagerank'] = pr[idx]\n",
    "for idx, c in enumerate(com):\n",
    "    for _ in c:\n",
    "        blob.vs[_]['blob_cluster'] = idx\n",
    "\n",
    "blob.save('data/blob2008_out.graphml', format='graphml') # export graph\n",
    "\n",
    "# now let's try each cluster separately\n",
    "\n",
    "subs = {}\n",
    "for idx, c in enumerate(com):\n",
    "    subs[idx] = blob.induced_subgraph(com[idx])\n",
    "    subs[idx]['name'] = '2008 blob subgraph ' + str(idx) + ' of ' + str(len(com))\n",
    "    # rerun the centrality, clustering, pagerank analyses\n",
    "    # (This could certainly use some functions for DRY principle)\n",
    "    bc = subs[idx].betweenness()\n",
    "    for idy, node in enumerate(subs[idx].vs):\n",
    "        subs[idx].vs[idy]['local_betweenness'] = bc[idy]\n",
    "    pr = subs[idx].pagerank()\n",
    "    for idy, node in enumerate(subs[idx].vs):\n",
    "        subs[idx].vs[idy]['local_pagerank'] = pr[idy]\n",
    "    subcom = subs[idx].community_leading_eigenvector(weights='weight', clusters=5)\n",
    "    for idy, c in enumerate(subcom):\n",
    "        for _ in c:\n",
    "            subs[idx].vs[_]['local_cluster'] = idy\n",
    "    filename = 'data/' + subs[idx]['name'].replace(' ', '_') + '_out.graphml'\n",
    "    subs[idx].save(filename, format='graphml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output/subgraph0_121n.png generated.\n",
      "output/subgraph1_2532n.png generated.\n",
      "output/subgraph2_168n.png generated.\n",
      "output/subgraph3_151n.png generated.\n",
      "output/subgraph4_34n.png generated.\n",
      "output/subgraph5_144n.png generated.\n"
     ]
    }
   ],
   "source": [
    "from igraph import layout, plot\n",
    "\n",
    "for _ in subs:\n",
    "    # delete label attributes to avoid visual clutter. In this example, the 'label'\n",
    "    del subs[_].vs['label']\n",
    "    layout = subs[_].layout_graphopt() # create a Fruchterman Reingold layout for the graph\n",
    "    filename = 'output/subgraph' + str(_) + '_' + str(len(subs[_].vs)) + 'n.png'\n",
    "    png = plot(subs[_], filename, layout=layout, bbox=(1000,1000), margin=100, edge_curved=True)\n",
    "    print(filename, 'generated.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After opening this 2008 graph in Gephi, we can manipulate colors and sizes of nodes to reflect attributes identified in igraph. Adjusting colors based on clustering, and node size based on betweenness centrality, provides a visual overview of the newly-identified features:\n",
    "\n",
    "![FD2008 graph from Gephi](data/2008_clusters_betweenness.png)\n",
    "\n",
    "We can observe thousands of small nodes with minimal betweenness value (over 8000 nodes in the graph have a betweenness of zero), and a handful of large nodes with higher values. The clusters are colored via cluster partition; one large cluster colored purple accounts for more than a third of the overall graph.\n",
    "\n",
    "Manual analysis of the large (high-betweenness) clusters indicates that these nodes represent users[[1](https://www.bing.com/search?q=site%3Aseclists.org+Valdis.Kletnieks+%28%29+vt+edu)][[2](https://www.bing.com/search?q=site%3Aseclists.org+3APA3A+%3C3APA3A+%28%29+SECURITY+NNOV+RU)] who are extremely active on the mailing list in multiple threads, partially confirming our hypothesis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TO DO: manual analysis of clusters;\n",
    "#        code to automate the process for multiple years;\n",
    "#        hypotheses for filtering/deleting clusters or nodes by attribute"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
